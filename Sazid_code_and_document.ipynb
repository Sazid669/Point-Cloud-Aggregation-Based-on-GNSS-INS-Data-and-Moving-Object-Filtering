{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating and Visualizing LiDAR Data from NuScenes\n",
    "#### Step 1: Initialize the Scene\n",
    "- Identify the target scene by its name (e.g., `scene-0103`) in the NuScenes dataset.\n",
    "- Retrieve the first sample token from the scene to start processing frames sequentially.\n",
    "\n",
    "**Reasoning**: To begin the aggregation process, the target scene must first be identified by name. The `first_sample_token` is then used to retrieve the starting point for sequential processing of frames in the scene. This ensures a systematic and complete traversal of all frames.\n",
    "\n",
    "#### Step 2: Iterate Through Frames\n",
    "- Use the `sample['next']` token to move through all frames in the scene.\n",
    "- For each frame:\n",
    "  - Retrieve the LiDAR data token (`LIDAR_TOP`).\n",
    "  - Load the raw point cloud using `LidarPointCloud.from_file()`.\n",
    "\n",
    "**Reasoning**: Iterating through the frames allows for the sequential processing of LiDAR data, ensuring that all data along the vehicle's trajectory is captured. By accessing the `sample['next']` token, the algorithm moves from one frame to the next until the entire scene is covered. This systematic approach ensures no data is missed.\n",
    "\n",
    "#### Step 3: Transform Points to Global Coordinates\n",
    "- Retrieve the sensor calibration data to compute the transformation from the sensor's local frame to the ego (vehicle) frame.\n",
    "  - Combine the sensor's translation and rotation into a transformation matrix.\n",
    "- Retrieve the ego pose data to compute the transformation from the ego frame to the global coordinate frame.\n",
    "  - Combine the ego vehicle's translation and rotation into another transformation matrix.\n",
    "- Multiply the two matrices to derive the transformation from the sensor frame to the global frame.\n",
    "- Apply this transformation to the LiDAR points using `.transform()`.\n",
    "\n",
    "**Reasoning**: LiDAR points are initially in the sensor's local coordinate system and need to be transformed into the global coordinate frame for proper alignment across frames. This is achieved through two transformations:\n",
    "- **Sensor to Ego Frame**: Using the sensor's calibration data, the points are transformed relative to the vehicle's coordinate system.\n",
    "- **Ego to Global Frame**: Using the vehicle's ego pose data, the points are further transformed into the global coordinate frame.\n",
    "Combining these transformations ensures that all points are accurately aligned in a common global frame.\n",
    "\n",
    "#### Step 4: Aggregate Transformed Points\n",
    "- Convert the transformed points into an Open3D `PointCloud` object.\n",
    "- Add the points to an aggregated point cloud initialized at the start.\n",
    "- Repeat this process for all frames until the end of the scene is reached.\n",
    "\n",
    "**Reasoning**: After transforming the points into the global frame, they are added to an aggregated point cloud. Using an Open3D `PointCloud` object simplifies the management and visualization of these points. Iterating through all frames and aggregating their points creates a comprehensive representation of the scene.\n",
    "\n",
    "\n",
    "#### Step 5: Save and Visualize\n",
    "- Save the aggregated point cloud as a `.pcd` file using `o3d.io.write_point_cloud()`.\n",
    "- Visualize the aggregated point cloud using Open3Dâ€™s visualization tools.\n",
    "\n",
    "**Reasoning**: Saving the aggregated point cloud as a `.pcd` file allows for further analysis and reuse of the data. Visualizing the point cloud using Open3D provides immediate feedback on the quality and completeness of the aggregation process, enabling verification of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.427 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n",
      "Saving aggregated point cloud with 1388928 points to step1_aggregated_cloud.pcd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "from nuscenes.utils.geometry_utils import transform_matrix, Quaternion\n",
    "\n",
    "\n",
    "def transform_lidar_to_global(nusc, lidar_token):\n",
    "    \"\"\"\n",
    "    Transform LiDAR point cloud data from sensor coordinates to global coordinates.\n",
    "\n",
    "    Args:\n",
    "        nusc (NuScenes): Instance of the NuScenes dataset.\n",
    "        lidar_token (str): Token for the LiDAR data.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Transformed point cloud as a (N, 3) array.\n",
    "    \"\"\"\n",
    "    # Get LiDAR metadata\n",
    "    lidar_data = nusc.get(\"sample_data\", lidar_token)\n",
    "    lidar_path = nusc.get_sample_data_path(lidar_token)\n",
    "\n",
    "    # Load LiDAR points\n",
    "    lidar_points = LidarPointCloud.from_file(lidar_path)\n",
    "\n",
    "    # Get ego pose and calibration data\n",
    "    ego_pose = nusc.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "    calibration = nusc.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "\n",
    "    # Compute transformations\n",
    "    sensor_to_ego = transform_matrix(\n",
    "        calibration[\"translation\"], Quaternion(calibration[\"rotation\"]), inverse=False\n",
    "    )\n",
    "    ego_to_global = transform_matrix(\n",
    "        ego_pose[\"translation\"], Quaternion(ego_pose[\"rotation\"]), inverse=False\n",
    "    )\n",
    "    sensor_to_global = ego_to_global @ sensor_to_ego\n",
    "\n",
    "    # Apply the global transformation\n",
    "    lidar_points.transform(sensor_to_global)\n",
    "\n",
    "    return lidar_points.points.T[:, :3]  # Return (N, 3) point cloud\n",
    "\n",
    "\n",
    "def aggregate_scene_points(nusc, scene_name):\n",
    "    \"\"\"\n",
    "    Aggregate all LiDAR frames in a scene into a single global point cloud.\n",
    "\n",
    "    Args:\n",
    "        nusc (NuScenes): Instance of the NuScenes dataset.\n",
    "        scene_name (str): Name of the target scene to process.\n",
    "\n",
    "    Returns:\n",
    "        o3d.geometry.PointCloud: Aggregated global point cloud.\n",
    "    \"\"\"\n",
    "    # Find the scene and initialize the global cloud\n",
    "    scene_info = next(scene for scene in nusc.scene if scene[\"name\"] == scene_name)\n",
    "    sample_token = scene_info[\"first_sample_token\"]\n",
    "    global_cloud = o3d.geometry.PointCloud()\n",
    "\n",
    "    while sample_token:\n",
    "        # Access the sample and get the LiDAR token\n",
    "        sample = nusc.get(\"sample\", sample_token)\n",
    "        lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "\n",
    "        # Transform LiDAR points to the global frame\n",
    "        lidar_points = transform_lidar_to_global(nusc, lidar_token)\n",
    "\n",
    "        # Convert to Open3D point cloud and add to the global cloud\n",
    "        lidar_cloud = o3d.geometry.PointCloud()\n",
    "        lidar_cloud.points = o3d.utility.Vector3dVector(lidar_points)\n",
    "        global_cloud += lidar_cloud\n",
    "\n",
    "        # Move to the next sample\n",
    "        sample_token = sample[\"next\"] if sample[\"next\"] else None\n",
    "\n",
    "    return global_cloud\n",
    "\n",
    "\n",
    "def visualize_point_cloud(point_cloud, window_title=\"Point Cloud Visualization\"):\n",
    "    \"\"\"\n",
    "    Visualize a point cloud using Open3D.\n",
    "\n",
    "    Args:\n",
    "        point_cloud (o3d.geometry.PointCloud): Point cloud to visualize.\n",
    "        window_title (str): Title for the visualization window.\n",
    "    \"\"\"\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(window_name=window_title)\n",
    "\n",
    "    # Configure rendering settings\n",
    "    render_option = vis.get_render_option()\n",
    "    render_option.point_size = 1.0\n",
    "\n",
    "    vis.add_geometry(point_cloud)\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize NuScenes dataset\n",
    "    nusc = NuScenes(version=\"v1.0-mini\", dataroot=\"Dataset/v1.0-mini\", verbose=True)\n",
    "\n",
    "    # Aggregate point clouds for a given scene\n",
    "    target_scene = \"scene-0103\"  # Specify the target scene\n",
    "    aggregated_cloud = aggregate_scene_points(nusc, target_scene)\n",
    "\n",
    "    # Save the aggregated point cloud\n",
    "    output_file = \"step1_aggregated_cloud.pcd\"\n",
    "    print(f\"Saving aggregated point cloud with {len(aggregated_cloud.points)} points to {output_file}\")\n",
    "    o3d.io.write_point_cloud(output_file, aggregated_cloud)\n",
    "\n",
    "    # Visualize the point cloud\n",
    "    visualize_point_cloud(aggregated_cloud, window_title=\"Aggregated Point Cloud\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering Moving and Static Objects in Aggregated Point Cloud\n",
    "\n",
    "#### Step 1: Compute Object Velocity\n",
    "- For each annotation, calculate the velocity of the object using its current and previous positions.\n",
    "  - Retrieve the current and previous translations from the annotation metadata.\n",
    "  - Calculate the displacement between the two positions.\n",
    "  - Compute the velocity by dividing the displacement by the time difference between the two frames.\n",
    "  - Return zero velocity if there is no previous annotation.\n",
    "\n",
    "**Reasoning**: Velocity is a critical factor in distinguishing moving objects from static ones. By comparing the current and previous positions of an object, we can calculate its displacement over time. Dividing the displacement by the time interval gives the velocity, which can be used to classify objects as moving or static. If no previous data is available, a velocity of zero is assigned to avoid false classification.\n",
    "\n",
    "\n",
    "#### Step 2: Filter Moving and Static Points\n",
    "- Initialize boolean masks for moving and static points in the point cloud.\n",
    "  - `moving_mask`: Marks points corresponding to moving objects.\n",
    "  - `static_mask`: Marks points corresponding to static objects.\n",
    "  - `ground_mask`: Identifies ground points based on a height threshold.\n",
    "\n",
    "- For each annotation:\n",
    "  - Retrieve the velocity of the object.\n",
    "  - Create a bounding box for the object using its translation, size, and rotation.\n",
    "  - Identify points within the bounding box and exclude ground points.\n",
    "  - Mark points as moving if the object's velocity exceeds the velocity threshold.\n",
    "  - Mark points as static otherwise.\n",
    "\n",
    "**Reasoning**: Points belonging to moving objects need to be identified and separated from the static environment. Boolean masks (`moving_mask`, `static_mask`, and `ground_mask`) are used to filter points based on their association with moving objects, static objects, or the ground plane. The ground mask ensures that low-lying ground points are excluded from object filtering. By using the bounding box of each object and comparing it to the aggregated points, we can accurately mark points as moving or static based on the velocity threshold.\n",
    "\n",
    "#### Step 3: Separate Moving and Static Clouds\n",
    "- Extract points from the aggregated cloud based on the masks:\n",
    "  - `moving_points`: Points corresponding to moving objects.\n",
    "  - `static_points`: Points corresponding to static objects.\n",
    "\n",
    "- Convert these points into Open3D `PointCloud` objects and assign distinct colors for visualization:\n",
    "  - Moving points: Red.\n",
    "  - Static points: Cyan.\n",
    "\n",
    "**Reasoning**: After identifying moving and static points, separating them into distinct clouds is essential for analysis and visualization. Open3D `PointCloud` objects are used to store and visualize these points. Assigning distinct colors (e.g., red for moving and cyan for static) enhances the visual distinction between moving and static elements in the scene, aiding interpretation.\n",
    "\n",
    "\n",
    "#### Step 4: Save and Visualize the Results\n",
    "- Save the static and moving point clouds as `.pcd` files.\n",
    "- Combine the two clouds and visualize them interactively using Open3D.\n",
    "\n",
    "**Reasoning**: Saving the static and moving point clouds as `.pcd` files ensures the results can be reused or analyzed in the future. Visualization provides immediate feedback, enabling verification of the filtering process and highlighting the separation between moving and static components. Combining the clouds and using an interactive viewer allows for a comprehensive overview of the scene.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.382 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import LidarPointCloud, Box\n",
    "from nuscenes.utils.geometry_utils import transform_matrix, points_in_box, Quaternion\n",
    "\n",
    "\n",
    "def compute_velocity(annotation, nusc):\n",
    "    \"\"\"\n",
    "    Calculate the velocity of an object from its annotation data.\n",
    "    \"\"\"\n",
    "    if annotation.get('prev'):\n",
    "        try:\n",
    "            # Calculate velocity using previous annotation\n",
    "            current_translation = np.array(annotation['translation'])\n",
    "            current_sample = nusc.get('sample', annotation['sample_token'])\n",
    "            current_timestamp = current_sample['timestamp']\n",
    "\n",
    "            prev_ann = nusc.get('sample_annotation', annotation['prev'])\n",
    "            prev_translation = np.array(prev_ann['translation'])\n",
    "            prev_sample = nusc.get('sample', prev_ann['sample_token'])\n",
    "            prev_timestamp = prev_sample['timestamp']\n",
    "\n",
    "            displacement = current_translation - prev_translation\n",
    "            time_delta = (current_timestamp - prev_timestamp) / 1e6  # Convert from microseconds to seconds\n",
    "            return np.linalg.norm(displacement) / time_delta\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError in compute_velocity: {e}\")\n",
    "            return 0.0\n",
    "    return 0.0  # Return zero if no previous annotation exists\n",
    "\n",
    "\n",
    "def filter_moving_objects(agg_pc, annotations, nusc, velocity_threshold=0.25, ground_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Identify points corresponding to moving objects in the aggregated point cloud.\n",
    "\n",
    "    Parameters:\n",
    "        agg_pc (np.ndarray): Aggregated global point cloud (Nx3).\n",
    "        annotations (list): List of annotations for the scene.\n",
    "        nusc (NuScenes): NuScenes instance for accessing metadata.\n",
    "        velocity_threshold (float): Minimum velocity to consider an object as moving.\n",
    "        ground_threshold (float): Height threshold to filter ground points.\n",
    "\n",
    "    Returns:\n",
    "        o3d.geometry.PointCloud: Static point cloud.\n",
    "        o3d.geometry.PointCloud: Moving point cloud.\n",
    "    \"\"\"\n",
    "    # Initialize masks\n",
    "    moving_mask = np.zeros(agg_pc.shape[0], dtype=bool)  # Initialize all as non-moving\n",
    "    static_mask = np.ones(agg_pc.shape[0], dtype=bool)   # Initialize all as static\n",
    "\n",
    "    # Ground points mask based on height threshold\n",
    "    ground_mask = agg_pc[:, 2] < ground_threshold  # Identify ground points (z < ground_threshold)\n",
    "\n",
    "    for ann in annotations:\n",
    "        if not ('translation' in ann and 'size' in ann and 'rotation' in ann):\n",
    "            print(f\"Skipping invalid annotation: {ann}\")\n",
    "            continue\n",
    "\n",
    "        velocity = compute_velocity(ann, nusc)\n",
    "\n",
    "        # Bounding box for the object\n",
    "        box = Box(\n",
    "            ann['translation'],\n",
    "            ann['size'],\n",
    "            Quaternion(ann['rotation'])\n",
    "        )\n",
    "\n",
    "        # Get points inside the bounding box\n",
    "        points_in_bbox = points_in_box(box, agg_pc.T)\n",
    "\n",
    "        # Exclude ground points from consideration\n",
    "        points_in_bbox = points_in_bbox & ~ground_mask\n",
    "\n",
    "        if velocity > velocity_threshold:\n",
    "            moving_mask[points_in_bbox] = True\n",
    "        else:\n",
    "            static_mask[points_in_bbox] = False\n",
    "\n",
    "    # Extract static and moving points\n",
    "    moving_points = agg_pc[moving_mask]\n",
    "    static_points = agg_pc[static_mask]\n",
    "\n",
    "    # Create Open3D point clouds\n",
    "    static_cloud = o3d.geometry.PointCloud()\n",
    "    static_cloud.points = o3d.utility.Vector3dVector(static_points)\n",
    "    static_cloud.paint_uniform_color([0.5, 0.9, 1]) \n",
    "\n",
    "    moving_cloud = o3d.geometry.PointCloud()\n",
    "    moving_cloud.points = o3d.utility.Vector3dVector(moving_points)\n",
    "    moving_cloud.paint_uniform_color([1, 0, 0])  # Red for moving\n",
    "\n",
    "    return static_cloud, moving_cloud\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize NuScenes\n",
    "    nusc = NuScenes(version='v1.0-mini', dataroot='Dataset/v1.0-mini', verbose=True)\n",
    "\n",
    "    # Load the aggregated point cloud\n",
    "    aggregated_cloud = o3d.io.read_point_cloud(\"step1_aggregated_cloud.pcd\")\n",
    "    agg_points = np.asarray(aggregated_cloud.points)\n",
    "\n",
    "    # Extract scene annotations\n",
    "    scene_name = \"scene-0103\"\n",
    "    scene = next(scene for scene in nusc.scene if scene['name'] == scene_name)\n",
    "    sample_token = scene['first_sample_token']\n",
    "    annotations = []\n",
    "\n",
    "    while sample_token:\n",
    "        sample = nusc.get('sample', sample_token)\n",
    "        for token in sample['anns']:\n",
    "            annotations.append(nusc.get('sample_annotation', token))\n",
    "        sample_token = sample['next'] if sample['next'] else None\n",
    "\n",
    "    # Filter moving and static objects\n",
    "    velocity_threshold = 0.25  # Velocity threshold for moving objects\n",
    "    ground_threshold = 0.3  # Height threshold for ground points\n",
    "    static_cloud, moving_cloud = filter_moving_objects(agg_points, annotations, nusc, velocity_threshold, ground_threshold)\n",
    "\n",
    "    # Save point clouds\n",
    "    o3d.io.write_point_cloud(\"static.pcd\", static_cloud)\n",
    "    o3d.io.write_point_cloud(\"moving.pcd\", moving_cloud)\n",
    "\n",
    "    # Visualize combined cloud\n",
    "    combined_cloud = static_cloud + moving_cloud\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(window_name=\"Aggregated Point Cloud with Moving Objects\")\n",
    "    render_option = vis.get_render_option()\n",
    "    render_option.point_size = 0.6\n",
    "    vis.add_geometry(combined_cloud)\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriching Static Cloud with Camera Colors in NuScenes\n",
    "\n",
    "#### Step 1: Load Camera Data\n",
    "- Retrieve camera data for the specified sample token and camera channel.\n",
    "  - Extract camera calibration, ego pose, and the image path from NuScenes metadata.\n",
    "\n",
    "**Reason**: To project points onto the image plane, we need camera-specific information. By extracting the calibration data (intrinsic and extrinsic), ego pose, and image path from NuScenes metadata, we can accurately map points from the global frame to the camera frame and associate them with the corresponding image pixels.\n",
    "\n",
    "#### Step 2: Transform Points to Camera Frame\n",
    "- Convert 3D points from the global coordinate frame to the camera frame.\n",
    "  - Compute the transformation matrix by combining camera pose and calibration data.\n",
    "  - Apply the transformation to the 3D points.\n",
    "\n",
    "**Reason**: 3D points exist in the global coordinate system, but the camera captures images in its local coordinate system. By applying the transformation matrices for the ego pose and the camera calibration, we can reposition the points relative to the camera's viewpoint, enabling correct projection onto the image plane.\n",
    "#### Step 3: Project Points to Image Plane\n",
    "- Project the 3D points in the camera frame to 2D pixel coordinates on the image plane.\n",
    "  - Use the intrinsic matrix of the camera for projection.\n",
    "  - Normalize by the depth of the points to get pixel coordinates.\n",
    "\n",
    "**Reason**: Cameras capture 3D scenes as 2D images. Using the intrinsic matrix of the camera, we project the transformed 3D points into 2D pixel coordinates. Normalizing by the depth ensures the projection accurately represents the points' locations on the image plane.\n",
    "\n",
    "#### Step 4: Enrich Points with Colors\n",
    "- For each valid 3D point:\n",
    "  - Verify it is within the camera's field of view.\n",
    "  - Extract its RGB value from the corresponding pixel in the image.\n",
    "  - Accumulate colors for points visible in multiple camera views.\n",
    "\n",
    "**Reason**: To create a visually enriched point cloud, each 3D point must be assigned an RGB color. By checking which points lie within the camera's field of view and depth range, we can associate valid points with pixel colors from the corresponding camera image. Accumulating colors from multiple camera views ensures better coverage and realistic representation.\n",
    "\n",
    "#### Step 5: Process Scene\n",
    "- Iterate through all samples in the scene and process the static cloud.\n",
    "  - For each sample, retrieve the associated camera images.\n",
    "  - Enrich the static cloud with RGB colors from these images.\n",
    "\n",
    "**Reason**: Scenes in NuScenes consist of multiple frames. Iterating through all samples ensures comprehensive processing of the static cloud, enriching it with colors from all relevant camera images. This step provides a holistic view of the scene's environment.\n",
    "\n",
    "#### Step 6: Combine and Visualize\n",
    "- Combine the enriched static cloud and the moving cloud.\n",
    "- Save the resulting point cloud as a `.pcd` file.\n",
    "- Visualize the enhanced point cloud interactively.\n",
    "\n",
    "**Reason**: Combining the enriched static cloud with the moving cloud creates a complete representation of the scene. Saving the combined point cloud allows for future analysis, and visualizing it provides immediate feedback on the quality of enrichment and alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 0.378 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22252/263384370.py:69: RuntimeWarning: invalid value encountered in divide\n",
      "  static_cloud.colors = o3d.utility.Vector3dVector(colors / visibility[:, None])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import open3d as o3d\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.geometry_utils import transform_matrix, Quaternion\n",
    "\n",
    "def load_camera_data(nusc, sample_token, camera_channel):\n",
    "    \"\"\"\n",
    "    Retrieve camera data, including calibration, ego pose, and image path.\n",
    "    \"\"\"\n",
    "    sample = nusc.get(\"sample\", sample_token)\n",
    "    cam_data = nusc.get(\"sample_data\", sample[\"data\"][camera_channel])\n",
    "    cam_calib = nusc.get(\"calibrated_sensor\", cam_data[\"calibrated_sensor_token\"])\n",
    "    cam_pose = nusc.get(\"ego_pose\", cam_data[\"ego_pose_token\"])\n",
    "    img_path = nusc.get_sample_data_path(cam_data[\"token\"])\n",
    "    return cam_calib, cam_pose, img_path\n",
    "\n",
    "def transform_to_camera_frame(points, cam_calib, cam_pose):\n",
    "    \"\"\"\n",
    "    Transform 3D points from the global frame to the camera frame.\n",
    "    \"\"\"\n",
    "    world_to_camera = np.linalg.inv(\n",
    "        transform_matrix(cam_pose[\"translation\"], Quaternion(cam_pose[\"rotation\"]), inverse=False) @\n",
    "        transform_matrix(cam_calib[\"translation\"], Quaternion(cam_calib[\"rotation\"]), inverse=False)\n",
    "    )\n",
    "    points_homogeneous = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    return (world_to_camera @ points_homogeneous.T).T\n",
    "\n",
    "def project_to_image(points_camera, intrinsic_matrix):\n",
    "    \"\"\"\n",
    "    Project 3D points in the camera frame to 2D pixel coordinates in the image.\n",
    "    \"\"\"\n",
    "    depth = points_camera[:, 2]\n",
    "    points_2d = (intrinsic_matrix @ points_camera[:, :3].T).T\n",
    "    points_2d[:, :2] /= depth[:, None]\n",
    "    return points_2d, depth\n",
    "\n",
    "def enrich_with_camera_colors(static_cloud, nusc, sample_token, camera_channels):\n",
    "    \"\"\"\n",
    "    Enrich the static cloud with RGB colors from multiple camera images.\n",
    "    \"\"\"\n",
    "    points = np.asarray(static_cloud.points)\n",
    "    colors = np.zeros((points.shape[0], 3))\n",
    "    visibility = np.zeros(points.shape[0])\n",
    "\n",
    "    for channel in camera_channels:\n",
    "        cam_calib, cam_pose, img_path = load_camera_data(nusc, sample_token, channel)\n",
    "        intrinsic = np.array(cam_calib[\"camera_intrinsic\"])\n",
    "        image = np.asarray(Image.open(img_path))\n",
    "\n",
    "        # Transform and project points\n",
    "        points_camera = transform_to_camera_frame(points, cam_calib, cam_pose)\n",
    "        points_2d, depth = project_to_image(points_camera, intrinsic)\n",
    "\n",
    "        # Filter valid points\n",
    "        valid_mask = (\n",
    "            (depth > 0) &\n",
    "            (points_2d[:, 0] >= 0) & (points_2d[:, 0] < image.shape[1]) &\n",
    "            (points_2d[:, 1] >= 0) & (points_2d[:, 1] < image.shape[0])\n",
    "        )\n",
    "\n",
    "        # Assign colors\n",
    "        valid_points_2d = points_2d[valid_mask, :2].astype(int)\n",
    "        if valid_mask.sum() > 0:\n",
    "            colors[valid_mask] += image[valid_points_2d[:, 1], valid_points_2d[:, 0]] / 255.0\n",
    "            visibility[valid_mask] += 1\n",
    "\n",
    "  \n",
    "    static_cloud.colors = o3d.utility.Vector3dVector(colors / visibility[:, None])\n",
    "    return static_cloud\n",
    "\n",
    "def process_scene(nusc, scene_name, static_file, moving_file, output_file):\n",
    "    \"\"\"\n",
    "    Process a scene by enriching static and moving clouds and combining them.\n",
    "    \"\"\"\n",
    "    static_cloud = o3d.io.read_point_cloud(static_file)\n",
    "    moving_cloud = o3d.io.read_point_cloud(moving_file)\n",
    "    moving_cloud.colors = o3d.utility.Vector3dVector(np.full((len(moving_cloud.points), 3), 0.5))  # Grayscale\n",
    "\n",
    "    # Process static cloud with camera images\n",
    "    scene = next(s for s in nusc.scene if s[\"name\"] == scene_name)\n",
    "    sample_token = scene[\"first_sample_token\"]\n",
    "\n",
    "    while sample_token:\n",
    "        sample = nusc.get(\"sample\", sample_token)\n",
    "        camera_channels = [key for key in sample[\"data\"].keys() if \"CAM\" in key]\n",
    "        static_cloud = enrich_with_camera_colors(static_cloud, nusc, sample_token, camera_channels)\n",
    "        sample_token = sample[\"next\"]  # Move to the next sample\n",
    "\n",
    "    # Combine and save the clouds\n",
    "    combined_cloud = static_cloud + moving_cloud\n",
    "    o3d.io.write_point_cloud(output_file, combined_cloud)\n",
    "\n",
    "    # Visualize the combined point cloud\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(window_name=\"Enhanced Point Cloud\")\n",
    "    render_options = vis.get_render_option()\n",
    "    render_options.background_color = np.array([0, 0, 0])\n",
    "    render_options.point_size = 1.0\n",
    "    vis.add_geometry(combined_cloud)\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nusc = NuScenes(version=\"v1.0-mini\", dataroot=\"Dataset/v1.0-mini\", verbose=True)\n",
    "    process_scene(\n",
    "        nusc,\n",
    "        scene_name=\"scene-0103\",\n",
    "        static_file=\"static.pcd\",\n",
    "        moving_file=\"moving.pcd\",\n",
    "        output_file=\"enhanced_cloud.pcd\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
